{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04317a-11de-451e-8a20-53adb0889c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pyod.utils.utility import precision_n_scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import column_or_1d\n",
    "from sklearn.utils.multiclass import type_of_target"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f26e211a-203a-4988-b30d-51af74490950",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data_seq = pd.read_pickle('data/BPIC15_1.pkl')\n",
    "data_seq = pd.read_pickle('data/env_permit.pkl')\n",
    "# display(data)\n",
    "data_seq['Intervals_seq']=[[round(val, 3) for val in inner_list] for inner_list in data_seq['Intervals_seq']]\n",
    "display(data_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ffd0b-2eaa-4ec0-a443-9ac65313a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902507f-2ba4-4adc-ab31-c822ed82d888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_obs_seq(df, perc_rows_2_modif, perc_items_2_modif):\n",
    "    # Randomly select x% of rows\n",
    "    num_rows_to_modify = int(len(df) * perc_rows_2_modif / 100)\n",
    "    rows_to_modify = np.random.choice(df.index, num_rows_to_modify, replace=False)\n",
    "\n",
    "    # Function to modify 'Obs_seq' for selected rows\n",
    "    def modify_sequence(seq):\n",
    "        # Calculate the number of items to modify in the sequence\n",
    "        num_items_to_modify = max(1, int(len(seq) * perc_items_2_modif / 100))\n",
    "        # Randomly select indices to modify in the sequence\n",
    "        indices_to_modify = np.random.choice(len(seq), num_items_to_modify, replace=False)\n",
    "\n",
    "        # Create random pairs of indices and swap their values\n",
    "        for i in range(0, len(indices_to_modify) - 1, 2):\n",
    "            seq[indices_to_modify[i]], seq[indices_to_modify[i + 1]] = (\n",
    "                seq[indices_to_modify[i + 1]], seq[indices_to_modify[i]]\n",
    "            )\n",
    "\n",
    "        return seq\n",
    "\n",
    "    # Modify 'Obs_seq' column for selected rows and create 'is_ano' column\n",
    "    df['is_ano'] = 0  # Initialize 'is_ano' column with 0\n",
    "\n",
    "    # Apply modification function to 'Obs_seq' for selected rows\n",
    "    df.loc[rows_to_modify, 'Obs_seq'] = df.loc[rows_to_modify, 'Obs_seq'].apply(modify_sequence)\n",
    "    df.loc[rows_to_modify, 'is_ano'] = 1  # Label modified rows with 1 in 'is_ano' column\n",
    "\n",
    "    return df\n",
    "\n",
    "def format_results(cv_results, mean_score_col, std_score):\n",
    "    results=pd.DataFrame.from_dict(clf.cv_results_)[[parameter_tested_col,mean_score_col,std_score]]\n",
    "    results['combined_scores'] = results.apply(lambda row: f\"{round(row[mean_score_col],3)} \\u00B1 ({round(row[std_score],3)})\", axis=1)\n",
    "    df_result = results[[parameter_tested_col, 'combined_scores']].T\n",
    "    df_result.columns = df_result.iloc[0]\n",
    "    df_result.columns.name = None\n",
    "    df_result = df_result[1:]\n",
    "    df_result.at[df_result.index[0], 'Dataset'] = file_name\n",
    "    df_result=df_result.set_index('Dataset')\n",
    "    return df_result\n",
    "# df2 = modify_obs_seq(data_seq,10,50)\n",
    "# display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49518ec2-2fb2-460a-982f-ea35729bc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seq2patterns import Seq2patterns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class MyAnomalyDetectionObjectFromPM(BaseEstimator, ClassifierMixin): \n",
    "    def __init__(self,nb_of_frequent_patterns, min_len_of_frequent_pattern, n_clust, seq_ano_perc, item_ano_perc, ano_method, algo_clustering, aggreg_method):\n",
    "        \n",
    "        # for pattern mining\n",
    "        self.nb_of_frequent_patterns = nb_of_frequent_patterns\n",
    "        self.min_len_of_frequent_pattern = min_len_of_frequent_pattern\n",
    "        self.kmeans_is_closed = None\n",
    "        self.n_clust = n_clust\n",
    "        self.algo_clustering=algo_clustering\n",
    "        self.aggreg_method=aggreg_method\n",
    "        \n",
    "        # for anomaly detection\n",
    "        self.seq_ano_perc = seq_ano_perc\n",
    "        self.item_ano_perc= item_ano_perc\n",
    "        self.ano_method = ano_method\n",
    "        \n",
    "        # model training object\n",
    "        self.seq2patterns_instance = None\n",
    "        self.max_len_seq=None\n",
    "        \n",
    "\n",
    "    def get_params(self, deep=True): \n",
    "        return {\n",
    "        \"nb_of_frequent_patterns\":self.nb_of_frequent_patterns,\n",
    "        \"min_len_of_frequent_pattern\":self.min_len_of_frequent_pattern,\n",
    "        # \"kmeans_is_closed\":self.kmeans_is_closed,\n",
    "        \"algo_clustering\":self.algo_clustering,\n",
    "        \"aggreg_method\":self.aggreg_method,\n",
    "        \"n_clust\":self.n_clust,\n",
    "        \"seq_ano_perc\":self.seq_ano_perc,\n",
    "        \"item_ano_perc\":self.item_ano_perc,\n",
    "        \"ano_method\":self.ano_method\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters): \n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self \n",
    "        \n",
    "       \n",
    "    def fit(self, X,y): \n",
    "        self.classes_ = np.unique(y, return_inverse=False)\n",
    "        \n",
    "        self.max_len_seq = max(X['Obs_seq'].apply(len))\n",
    "        \n",
    "        X = X[[\"Entite\", \"Obs_seq\",\"Intervals_seq\"]] \n",
    "        # y=None\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.ano_method in ['WCFPOF','WCFPOF_clust']:\n",
    "            self.kmeans_is_closed==True\n",
    "        else:\n",
    "            self.kmeans_is_closed==False\n",
    "        \n",
    "        if self.ano_method in ['FPOF','WCFPOF','LFPOF']:\n",
    "            self.n_clust=1\n",
    "            \n",
    "        self.seq2patterns_instance = Seq2patterns(nb_of_frequent_patterns = self.nb_of_frequent_patterns, \n",
    "                                                  min_len_of_frequent_pattern = self.min_len_of_frequent_pattern, \n",
    "                                                  kmeans_is_closed = self.kmeans_is_closed, \n",
    "                                                  n_clust = self.n_clust,\n",
    "                                                  algo_clustering = self.algo_clustering,\n",
    "                                                  aggreg_method=self.aggreg_method)\n",
    "        self.seq2patterns_instance.fit(X)\n",
    "        return 'fitted'\n",
    "\n",
    "    def decision_function(self, X): \n",
    "\n",
    "        patterns_X = self.seq2patterns_instance.transform(X)\n",
    "        # pour la jointure après on utilise comme clé le numéro de row, càd l'index.\n",
    "        X.reset_index(inplace=True)  # Resetting the index\n",
    "        X['Entite'] = X.index  # Creating 'Entite' column using reset index values\n",
    "\n",
    "\n",
    "        \n",
    "        # add the empty pattern to all cases in the patterns table (used to return all cases, not only those with frequent patterns)\n",
    "        case_distinct_values_list = X['Entite'].unique().tolist()\n",
    "\n",
    "        # Create an empty list to store dictionaries for new rows\n",
    "        new_rows = []\n",
    "        \n",
    "        # Loop through each distinct value and create a dictionary for each value\n",
    "        for case in case_distinct_values_list:\n",
    "            new_row = {'Combination': 'Null_Patn', 'seqIndex': case, 'Cluster': 0, 'freq': 1}\n",
    "            new_rows.append(new_row)  # Append the dictionary to the list\n",
    "\n",
    "        # Concatenate the list of dictionaries with the existing DataFrame using pd.concat()\n",
    "        if new_rows:\n",
    "            patterns_X = pd.concat([patterns_X, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "                \n",
    "\n",
    "            \n",
    "        X[\"len_seq\"]=X['Obs_seq'].apply(len) # add len of sequence column\n",
    "        patterns_X = patterns_X.merge(X[['len_seq']], left_on='seqIndex', right_index=True, how='left') # add it to results for further calculations\n",
    "        \n",
    "        patterns_X['len_pattern'] = patterns_X['Combination'].apply(lambda x: len(str(x).split('[')[1].split(']')[0].split(',')) if '[' in str(x) and ']' in str(x) else 0)\n",
    "        \n",
    "        var_sum_freq = patterns_X[['Combination', 'freq']].drop_duplicates()['freq'].sum()\n",
    "        max_len_seq2 = max(self.max_len_seq,max(X['Obs_seq'].apply(len)))\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if self.ano_method == 'FPOF': # sum freq/nb total patterns\n",
    "                return patterns_X.groupby('seqIndex').apply(lambda x: 1-(x['freq'].sum()/((self.nb_of_frequent_patterns)+1))).tolist() # +1 car on a le pattern vide\n",
    "            \n",
    "            elif self.ano_method == 'FPOF_clust': # sum freq/nb total patterns\n",
    "                return patterns_X.groupby('seqIndex').apply(lambda x: 1-(x['freq'].sum()/((self.nb_of_frequent_patterns)+1))).tolist() # +1 car on a le pattern vide\n",
    "            \n",
    "            elif self.ano_method == 'WCFPOF': # sum freq/nb total patterns(avec patterns fermés)\n",
    "                # return patterns_X.groupby('seqIndex').apply(lambda x: 1-(x['freq'].sum()/((self.nb_of_frequent_patterns)+1))).tolist() # +1 car on a le pattern vide\n",
    "                return patterns_X.groupby('seqIndex').apply(lambda x: 1 - ((x['freq'] * (x['len_pattern'] / x['len_seq'])).sum() / (self.nb_of_frequent_patterns + 1))).tolist()\n",
    "            \n",
    "            elif self.ano_method == 'WCFPOF_clust': # sum freq/nb total patterns(avec patterns fermés)\n",
    "                # return patterns_X.groupby('seqIndex').apply(lambda x: 1-(x['freq'].sum()/((self.nb_of_frequent_patterns)+1))).tolist() # +1 car on a le pattern vide\n",
    "                return patterns_X.groupby('seqIndex').apply(lambda x: 1 - ((x['freq'] * (x['len_pattern'] / x['len_seq'])).sum() / (self.nb_of_frequent_patterns + 1))).tolist()\n",
    "       \n",
    "        except ValueError:\n",
    "            print(\"ERREUR sur les méthodes de calcul de score d'anomalie\")\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict_proba(self, X): \n",
    "        \n",
    "        return self.decision_function(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        def create_top_x_percent_list(input_list, x):\n",
    "            sorted_list = sorted(input_list, reverse=True)\n",
    "            threshold_index = int(len(sorted_list) * (x / 100))\n",
    "            threshold_value = sorted_list[threshold_index]\n",
    "\n",
    "            new_list = [1 if val >= threshold_value else 0 for val in input_list]\n",
    "            return new_list\n",
    "        \n",
    "        return create_top_x_percent_list(self.decision_function(X), self.seq_ano_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f36c82-072a-4e1a-a8b0-a2189bc96c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "list_dataset = [\n",
    "\n",
    "    \"Helpdesk\",\n",
    "    \"BPI_Challenge_2012_A\",\n",
    "    \"BPI_Challenge_2012_O\",\n",
    "    \"BPI_Challenge_2013_closed_problems\",\n",
    "    \"bpi_challenge_2013_incidents\"\n",
    "    ]\n",
    "\n",
    "final_results_precision=None;final_results_recall=None;final_results_f1=None # Pour les résultats finaux\n",
    "\n",
    "for file_name in list_dataset:\n",
    "   \n",
    "    start_time = datetime.now()\n",
    "    print(start_time)\n",
    "    \n",
    "    print(\"-------------------------------------------------\"+file_name+\"-------------------------------------------------\")\n",
    "    data = pd.read_pickle(r'data/%s.pkl'%(file_name))\n",
    "    \n",
    "    # display(data)\n",
    "    \n",
    "    # we may delete the sequences that are too short or too long\n",
    "    # data = data[data['Obs_seq'].apply(len) >= 1]\n",
    "    # data = data[data['Obs_seq'].apply(len) <= 500]\n",
    "    \n",
    "    \n",
    "    # we may troncate right the sequences\n",
    "    # data['Obs_seq'] = data['Obs_seq'].apply(lambda x: x[-20:])\n",
    "    # data['Intervals_seq'] = data['Intervals_seq'].apply(lambda x: x[-20:])\n",
    "    # data['Intervals_seq'] = [[round(val, 2) for val in inner_list] for inner_list in data['Intervals_seq']]\n",
    "    # print(\"longeur dataset:\"+str(len(data)))\n",
    "    \n",
    "    # fin ajout\n",
    "    data['Entite'] = data.index\n",
    "    # display(data)\n",
    "    \n",
    "    # paramètres par défaut\n",
    "    perc_rows_2_modif=10\n",
    "    perc_items_2_modif=50\n",
    "\n",
    "\n",
    "    scoring = {\n",
    "               \"precision\":\"precision\",\n",
    "               \"f1\": \"f1\",\n",
    "               \"recall\": \"recall\"\n",
    "              }\n",
    "\n",
    "    # parameters={'ano_method':['FPOF','WCFPOF','FPOF_clust','WCFPOF_clust']}\n",
    "    # parameters={'aggreg_method':['min','max','mean']}\n",
    "    # parameters={'n_clust':[1,2,3,4,5,6,7,8,9,10]}\n",
    "    # parameters={'min_len_of_frequent_pattern':[2,3,4,5,6]}\n",
    "    # parameters={'nb_of_frequent_patterns':[0.5,0.6,0.7,0.8,0.9,0.9999]}\n",
    "    parameters={'nb_of_frequent_patterns':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n",
    "    \n",
    "\n",
    "    parameter_tested_col='param_'+list(parameters.keys())[0] # to print at the end\n",
    "    \n",
    "    # création instance classe\n",
    "    MyModel = MyAnomalyDetectionObjectFromPM(nb_of_frequent_patterns=10,\n",
    "                                        min_len_of_frequent_pattern=3,\n",
    "                                        n_clust=4,\n",
    "                                        seq_ano_perc=perc_rows_2_modif, \n",
    "                                        item_ano_perc=perc_items_2_modif, \n",
    "                                        ano_method='WCFPOF_clust',\n",
    "                                        algo_clustering=\"kmeans\",\n",
    "                                        aggreg_method=\"min\")\n",
    "    # création instance gridsearch\n",
    "    clf = GridSearchCV(MyModel,\n",
    "                       cv=StratifiedKFold(shuffle=True,\n",
    "                                           n_splits=5,\n",
    "                                           random_state=1),\n",
    "                       scoring=scoring,\n",
    "                       error_score=\"raise\",\n",
    "                       n_jobs=6,\n",
    "                       refit=False,\n",
    "                       verbose=0,\n",
    "                       param_grid=parameters)\n",
    "    \n",
    "    # to prepare data\n",
    "    data_prepared = modify_obs_seq(data,perc_rows_2_modif,perc_items_2_modif)\n",
    "\n",
    "    clf.fit(data_prepared.loc[:, data_prepared.columns != 'is_ano'], data_prepared['is_ano'])\n",
    "\n",
    "\n",
    "\n",
    "    df_result_precision=format_results(clf.cv_results_, \"mean_test_precision\", \"std_test_precision\")\n",
    "    df_result_recall=format_results(clf.cv_results_, \"mean_test_recall\", \"std_test_recall\")\n",
    "    df_result_f1=format_results(clf.cv_results_, \"mean_test_f1\", \"std_test_f1\")\n",
    "    \n",
    "    if final_results_precision is not None:\n",
    "        final_results_precision = pd.concat([final_results_precision,df_result_precision])\n",
    "        final_results_recall = pd.concat([final_results_recall,df_result_recall])\n",
    "        final_results_f1 = pd.concat([final_results_f1,df_result_f1])\n",
    "    else:\n",
    "        final_results_precision=df_result_precision\n",
    "        final_results_recall=df_result_recall\n",
    "        final_results_f1=df_result_f1\n",
    "\n",
    "    display(final_results_precision.style.set_caption('Precision_score'))\n",
    "    display(final_results_recall.style.set_caption('Recall_score'))\n",
    "    display(final_results_f1.style.set_caption('F1_score'))\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
